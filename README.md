# Projeto Construcao do Algoritmo FOP
Trago um exemplo simples de construção de algorithmo de regressão linear, baseado no Paper de autoria de Ted Moskovitz, Rui Wang, Janice Lan, Sanyam Kapoor, Thomas Miconi, Jason Yosinski, Aditya Rawal da Uber AI. Link https://arxiv.org/abs/1910.08461.
Neste Paper é abordado um método denominado de pré-condicionamento de primeira ordem (FOP). A FOP não tenta calcular um pré-condicionador específico, como o Hessian inverso, mas usa descida de hipergradiente de primeira ordem (Maclaurin et al., 2015) para aprender uma transformação adaptável on-line diretamente a partir da função objetivo da tarefa. Este método adiciona um valor computacional mínimo e custo de memória para configurações padrão de treinamento em rede profunda e resulta em melhor convergência velocidade e generalização em comparação com abordagens padrão. A FOP pode ser aplicada de forma flexível a qualquer problema de otimização baseado em gradiente.
